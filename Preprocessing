######################################################################
#Start Program
######################################################################
library(tidyverse)
library(deplyr)
library(naniar)
#library(visdat)
library(funModeling)
library(Hmisc)
library(ggplot2)
library(corrplot)
#library(GGally)
library(outliers)
library(mice)
library(VIM)
library(lattice)
library(binaryLogic)
library(moments)
# library("FactoMineR")
# library("factoextra")
library(gbm)
library(formattable)
#library("umap")

load.lib<-c("tidyverse", "naniar","ggplot2","corrplot","outliers", "funModeling",
            "Hmisc", "mice","VIM","binaryLogic","moments", "caret", "gbm", "formattable")


install.lib<-load.lib[!(load.lib %in% installed.packages())]
for(lib in install.lib) install.packages(lib,dependencies=TRUE)
sapply(load.lib,require,character=TRUE)

rm(install.lib, lib, load.lib)


######################################################################
##Functions
######################################################################

read <- function(filename){
  ScoreCardRawData <- read.csv(filename,encoding="UTF-8",stringsAsFactors = FALSE, na.strings = c("PrivacySuppressed", "NULL", "", " "))
  ##Choose fields
  ScoreCardRawData <- ScoreCardRawData %>% select(median_hh_inc,poverty_rate,
                                            unemp_rate, female, md_faminc, INSTNM, Year, STABBR, PREDDEG, CONTROL, LOCALE,
                                            locale2, HIGHDEG, sch_deg, region, CCBASIC, CCSIZSET, ADM_RATE, PCTFLOAN, SAT_AVG, SATVRMID, SATMTMID, SATWRMID, 
                                            ACTCMMID, ACTENMID, ACTMTMID, ACTWRMID, COSTT4_A, COSTT4_P, TUITIONFEE_IN, TUITIONFEE_OUT, 
                                            TUITIONFEE_PROG, TUITFTE, INEXPFTE, DEBT_MDN_SUPP, COMP_ORIG_YR2_RT, COMP_4YR_TRANS_YR2_RT, 
                                            COMP_2YR_TRANS_YR2_RT, AVGFACSAL, INC_PCT_LO, INC_PCT_M1, INC_PCT_M2, INC_PCT_H1, INC_PCT_H2, 
                                            PAR_ED_PCT_1STGEN, DEP_STAT_PCT_IND, md_earn_wne_p10)
  return(ScoreCardRawData)
}

datatype_Classification <- function(dataset){
  ##Looking at the data
  # print("Summary Before making changes to the datatype")
  # print(summary(dataset))
  
  ##Converting character fields to factors and integers to numeric
  dataset <- dataset %>% mutate_if(sapply(dataset, is.character), as.factor)
  dataset <- dataset %>% mutate_if(sapply(dataset, is.integer), as.numeric)
  #dataset <- dataset %>% mutate(Year = as.factor(Year))
  dataset <- dataset %>% mutate(sch_deg = as.factor(sch_deg))
  ##Looking at the data
  # print("Summary after making changes to the datatype")
  # print(summary(dataset))
  return(dataset)
}



attendance_Cost <- function(dataset){
  ##Combining the attendance cost fields from program year and academic year institutions into one field
  ##Populating 0 when COSTT4_A is available and COSTT4_P is NA and vice versa
  dataset$COSTT4_A[is.na(dataset$COSTT4_A) & !is.na(dataset$COSTT4_P)] <- 0
  dataset$COSTT4_P[is.na(dataset$COSTT4_P) & !is.na(dataset$COSTT4_A)] <- 0
  dataset <- dataset %>% mutate(ATDCOST = COSTT4_A + COSTT4_P)
  
  ##Removing old cost features and adjusting the field positions
  dataset <- dataset[, !names(dataset) %in% c("COSTT4_A", "COSTT4_P")]
  return(dataset)
}

NencodeState<-function(){
  print("Encoding the state1 field into longitude & latitude fields")
  #dataset$longitude<-NA
  #dataset$latitude<-NA
  states<-data.frame(STABBR=state.abb,
                     longitude=state.center$x,
                     latitude=state.center$y,
                     #region=state.region,
                     stringsAsFactors = FALSE)
  # Some datasets use some abbreviations not in the above table
  # DC for District of Columbia
  # AP - Armed Forces Pacific - Used Kyoto, Japan as coordinates
  # AE - Armed Forces Europe - Used Berlin, Germany as coordinates
  # PR - Puerto Rico, used San Juan, Puerto Rico
  otherStates<-list(
    data.frame(STABBR="DC",longitude=-77.009003,latitude=38.889931),
    data.frame(STABBR="AP",longitude=-135.75385,latitude=35.02107),
    data.frame(STABBR="AE",longitude=-52.520008,latitude=313.404954),
    data.frame(STABBR="PR", longitude=-66.105721,latitude=18.466333)
  )
  # Convert the above list to a dataframe and then join into a single dataframe
  states<-rbind(states,do.call(rbind.data.frame,otherStates))
  return(states)
}

state_to_latlong <- function(dataset){
  state <- NencodeState()
  dataset <- merge(dataset,state,by="STABBR")
  print(paste0("There are ", nrow(dataset[is.na(dataset$longitude),]), " states without latitude or longitude"))
  dataset <- dataset[, names(dataset) != "STABBR"]
  return(dataset)
}

data_Transformation <- function(dataset){
  dataset <- dependency_Test(dataset)
  dataset <- datatype_Classification(dataset)
  dataset <- attendance_Cost(dataset)
  dataset <- state_to_latlong(dataset)
  dataset <- dataset %>% mutate(SalaryClass = equal_freq(dataset$md_earn_wne_p10, n_bins = 4))
  ##adjusting the field positions
  dataset <- dataset[, c((1:20), (44:46), (21:43), 47)]
}

dependency_Test <- function(dataset){
  #print("State and ")
  chi <- chisq.test(table(dataset$region, dataset$STABBR),correct = FALSE)
  if(chi$p.value<0.5){
    print("The State and Region fields are dependent on each other. Hence removing the region field from the dataset.")
  }
  dataset <- dataset[, names(dataset) != "region"]
}

handling_Missing_Data <- function(dataset){
  #Removing records with all NA values in output fields and cost_of_education fields
  NA_Summary <- aggregate(is.na(dataset), list(dataset$Year), mean)
  Chosen_Years <- subset(NA_Summary, md_earn_wne_p10 != 1 & (ATDCOST != 1 & ADM_RATE != 1))
  years <- Chosen_Years[,1]
  print("The following years have valid values in the output fields and few other input fields:")
  print(years)
  ChosenData_Lesser_Years <- subset(dataset, (Year %in% years))
  
  ##Removing Records with NA values in the target field
  ChosenData_Lesser_Years <- subset(ChosenData_Lesser_Years, (!is.na(md_earn_wne_p10)))
  
  ##Counting NA Columnwise
  print("Checking the NA percentage in each column")
  print(colSums(is.na(ChosenData_Lesser_Years)))
  
  ##Removing fields that have more than 70% NA values
  ChosenData_Lesser_Fields <- ChosenData_Lesser_Years[colSums(is.na(ChosenData_Lesser_Years))<=(nrow(ChosenData_Lesser_Years)*0.7)]
  
  ##Counting NA Columnwise
  print("After removing the fields with higher percentage of NA values")
  print(colSums(is.na(ChosenData_Lesser_Fields)))
  
  ##Removing records with higher percentage of NULL fields in a row
  ChosenData_Lesser_Fields$countNA <- rowSums(is.na(ChosenData_Lesser_Fields))
  
  Rows_Missing_Data <- data.frame(table(ChosenData_Lesser_Fields$countNA))
  names(Rows_Missing_Data) <- c("NACount", "Freq")
  Rows_Missing_Data$CumFreq <- cumsum(Rows_Missing_Data$Freq)
  mp <- ggplot(Rows_Missing_Data, aes(x = NACount, y = CumFreq, group = 1)) +
    geom_line() +
    geom_point(size = 2) + 
    geom_text(aes(label = CumFreq), hjust = 0, vjust = 1.5)
  
  print(mp)
  
  #Filtering the rows with more than 9 NA values and removing the additional field
  ChosenData_Lesser_Rows <- ChosenData_Lesser_Fields[ChosenData_Lesser_Fields$countNA <= 9, c(-(1:2), -30)]
  return(ChosenData_Lesser_Rows)
}

Correlation <- function(dataset){
  Numeric_Fields <- dataset[, !names(dataset) %in% c("SalaryClass", "md_earn_wne_p10", "sch_deg", "PREDDEG", "CONTROL", "HIGHDEG")]
  
  ##Correlation between numeric input fields and the output field
  Output.cor = cor(Numeric_Fields, dataset[,22], method = c("pearson"),  use='pairwise.complete.obs')
  oc <- corrplot(Output.cor, method="color",
                 col=brewer.pal(n=10, name="RdYlBu"),
                 addCoef.col = "black",
                 tl.col="black",
                 tl.cex = 0.9, 
                 tl.srt=90)
  
  print(oc)
  
  ##Removing input fields that are weakly correlated with the Target field
  dataset <- dataset [, !names(dataset) %in% c("COMP_4YR_TRANS_YR2_RT", "COMP_2YR_TRANS_YR2_RT", "ADM_RATE")]
  
  ##Correlation between numeric input fields
  Input.cor = cor(Numeric_Fields, method = c("pearson"),  use='pairwise.complete.obs')
  
  ic <- corrplot(Input.cor, method="color",
                 col=brewer.pal(n=10, name="RdYlBu"),
                 type="lower",
                 addCoef.col = "black",
                 tl.col="black",
                 tl.cex = 0.9, 
                 tl.srt=90, 
                 diag=FALSE)
  
  print(ic)
  return(dataset)
}

getmode <- function(v) {
  uniqv <- unique(v)
  uniqv[which.max(tabulate(match(v, uniqv)))]
}


imputing_With_MedianandMode <- function(dataset){
  for(i in 1:ncol(dataset)-1){
    #Imputing Values with mean for numeric values
    if (is.numeric(dataset[,i])){
      dataset[is.na(dataset[,i]), i] <- median(dataset[,i], na.rm = TRUE)}
    #Imputing Values with mode for categorical values
    else if (is.factor(dataset[,i])){
      dataset[is.na(dataset[,i]), i] <- getmode(dataset[,i])}
  }
  return(dataset)
}

imputing_With_Mice <- function(dataset){
  
  aggr_plot <- aggr(dataset, col=c('yellow','grey'), numbers=TRUE, sortVars=TRUE, labels=names(data), cex.axis=.7, gap=3, ylab=c("Histogram of missing data","Pattern"))
  
  #marginplot(ChosenData_Lesser_Rows[c()])
  
  mice_mod <- mice::mice(dataset, m=3, maxit=3, method = "cart")
  print(mice_mod)
  print(densityplot(mice_mod))
  ChosenData_Imputed <- complete(mice_mod)

  #stripplot(mice_mod, pch = 20, cex = 1.2)
  
  return(ChosenData_Imputed)
}

checkOutlier <- function(dataset){
  par(mfrow = c(3, 5)) 
  for(name in names(dataset)){
    Input_Variable <- dataset[[name]]
    Input_Outliers <- boxplot.stats(Input_Variable)$out
    Input_Outlier_Ind <- which(Input_Variable %in% c(Input_Outliers))
    outpct = as.numeric(nrow(dataset[Input_Outlier_Ind, ])/nrow(dataset))
    #print(paste0("The outliers in the ", name, " field fall in the following quantiles"))
    #print(quantile(Input_Outliers))
    if(outpct > 0){
      print(paste0("Percentage of outliers in ",name, " field is ", outpct))
      print(boxplot(Input_Variable, xlab = name))
      dataset[,name] <- capOutlier(dataset[,name])
    }
  }
  par(mfrow=c(1,1))
  return(dataset)
}

capOutlier <- function(x){
  qnt <- quantile(x, probs=c(.25, .75), na.rm = T)
  caps <- quantile(x, probs=c(.05, .95), na.rm = T)
  H <- 1.5 * IQR(x, na.rm = T)
  x[x < (qnt[1] - H)] <- caps[1]
  x[x > (qnt[2] + H)] <- caps[2]
  return(x)
}

output_Outliers <- function(dataset){
  ##Analysing the presense of outliers in the output variable
  op <- ggplot(dataset) +
    aes(x = md_earn_wne_p10) +
    geom_histogram(bins = 30L, fill = "#0c4c8a") +
    theme_minimal()
  
  print(op)
  
  #boxplot(ChosenData_Imputed$md_earn_wne_p10, ylab = "md_earn_wne_p10")
  Target_Outliers <- boxplot.stats(dataset$md_earn_wne_p10)$out
  Target_Outlier_Ind <- which(dataset$md_earn_wne_p10 %in% c(Target_Outliers))
  print(paste0("Percentage of outliers in target variable: ", as.numeric(nrow(dataset[Target_Outlier_Ind, ])/nrow(dataset))))
  print(nrow(dataset[Target_Outlier_Ind, ]))
}

encoding <- function(dataset){
  dmy <- dummyVars(" ~ .", data = dataset)
  dataset_Onehot <- data.frame(predict(dmy, newdata = dataset))
  return(dataset_Onehot)
}

scaling <- function(dataset){
  return(scale(dataset)) ##Standardization
}


final_Dataset <- function(ML_Model, dataset){
  if(ML_Model == 'NN'){
    Preprocessed_dataset <- encoding(dataset)
    #Preprocessed_dataset <- scaling(Preprocessed_dataset)
  }else if(ML_Model %in% c('GBM', 'RF')){
    Preprocessed_dataset <- encoding(dataset[, !names(dataset) %in% c("SalaryClass")])
    Preprocessed_dataset$SalaryClass <- dataset$SalaryClass
  }
  return(Preprocessed_dataset)
}


######################################################################
##GBM for preprocessing decisions
######################################################################


gbm_regression <- function(train_data, test_data){
  # GBM function parameters
  DISTRIBUTION <- "gaussian"
  NTREES <- 5000
  FOLDS <- 5
  LEARNING_RATE <- 0.1
  
  input_fields <- paste(names(train_data)[which(names(train_data)!=OUTPUTFIELD_REGRESSION)], collapse = "+")
  formula <- as.formula(paste(OUTPUTFIELD_REGRESSION,"~",input_fields))
  
  # Train the model
  model <- gbm(
    formula = formula
    ,data = train_data
    ,distribution = DISTRIBUTION
    ,n.trees = NTREES
    ,shrinkage = LEARNING_RATE
    ,cv.folds = FOLDS
  )
  
  print("****************************************")
  print(model)
  
  print("Printed influence of predictors.")
  importance = summary.gbm(model, plotit=TRUE)
  rownames(importance) <- NULL
  print(formattable::formattable(importance))
  
  cv_ntrees = gbm.perf(model, method = "cv")
  
  
  # Evaluate the model by testing data
  output <- predict.gbm(
    object = model
    ,newdata = test_data
    ,n.trees = cv_ntrees
    ,type = "response"
  )
  
  actual <- test_data[,OUTPUTFIELD_REGRESSION]
  predict <- output
  
  # Calculate MAE
  MAE <- round(mae(actual, predict),2)
  print(paste("MAE of testing data:", MAE))
  
  # Calculate RMSE
  RMSE <- round(rmse(actual, predict),2)
  print(paste("RMSE of testing data:", RMSE))
  
  # Calculate rsquared
  R2 <- round(r2(actual, predict),2)
  print(paste("R Squared of testing data:", R2))
  
}

cleanDataset <- function(dataset, type){
  if (type == "regression"){
    drops <- c("",OUTPUTFIELD_CLASSIFICATION)
  }
  else if (type == "classification"){
    drops <- c("",OUTPUTFIELD_REGRESSION)
  }
  
  cleanData <- dataset[ , !(names(dataset) %in% drops)]
  return(cleanData)
}

randomize_dataset <- function(dataset){
  training_records<-round(nrow(dataset)*(70/100))
  training_data <- dataset[1:training_records,]
  testing_data = dataset[-(1:training_records),]
  
  return(list(training_data = training_data, testing_data = testing_data))
}

mae <- function(actual, predict){
  return(mean(abs(actual-predict)))
}

rmse <- function(actual, predict){
  return(sqrt(mean((actual-predict)^2)))
}

r2 <- function(actual, predict){
  return(1-(sum((actual-predict)^2)/sum((actual-mean(actual))^2)))
}

OUTPUTFIELD_REGRESSION <- "md_earn_wne_p10"
OUTPUTFIELD_CLASSIFICATION <- "SalaryClass"

applyGBM_Reg <- function(dataset){
  
  Preprocessed_Dataset<-final_Dataset('GBM', dataset)
  #print(head(Preprocessed_Dataset))
  Preprocessed_Dataset<-Preprocessed_Dataset[order(runif(nrow(Preprocessed_Dataset))),]
  
  regression_dataset <- cleanDataset(Preprocessed_Dataset, "regression")
  regression_dataset <- randomize_dataset(regression_dataset)
  
  gbm_regression <- gbm_regression(regression_dataset$training_data, regression_dataset$testing_data)
  
}


######################################################################
##Main Code
######################################################################

DATASET_FILENAME  <- "Scorecard.csv"
setwd("C:/Users/rhmou/OneDrive/Documents/Practical Business Analytics/Coursework/ScoreCardDataset")

ScoreCardRawData <- read(DATASET_FILENAME)
#head(ScoreCardRawData)

ChosenData <- data_Transformation(ScoreCardRawData)

#head(ChosenData)

ChosenData_LesserNA <- handling_Missing_Data(ChosenData)

ChosenData_Lesser_Fields <- Correlation(ChosenData_LesserNA)

##imputing
#freq(ChosenData)
describe(ChosenData_Lesser_Fields)
plot_num(ChosenData_Lesser_Fields)

ChosenData_MedianModeImputed <- imputing_With_MedianandMode(ChosenData_Lesser_Fields)

#freq(ChosenData_MeanModeImputed)
describe(ChosenData_Lesser_Fields)
plot_num(ChosenData_MedianModeImputed)

applyGBM_Reg(ChosenData_MedianModeImputed)

ChosenData_MiceImputed <- imputing_With_Mice(ChosenData_Lesser_Fields)

describe(ChosenData_MiceImputed)
plot_num(ChosenData_MiceImputed)

applyGBM_Reg(ChosenData_MiceImputed)

ChosenData_Outliers <- ChosenData_MiceImputed[, c(-(1:4), -(23:24))]
ChosenData_Outliers <- checkOutlier(ChosenData_Outliers)
ChosenData_Capped <- ChosenData_MiceImputed %>% mutate(ChosenData_Outliers)

##Checking presence of outliers in the output field
output_Outliers(ChosenData_Capped)

applyGBM_Reg(ChosenData_Capped)

##To write the preprocessed dataset into csv - use model name(RF, GBM, NN) and the dataset
nn_PP_Dataset <- final_Dataset('NN', ChosenData_Capped)
write.csv(nn_PP_Dataset, "nn_PP_Dataset", row.names = FALSE)
